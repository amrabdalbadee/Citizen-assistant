# Citizen Support Assistant - Environment Configuration
# 100% FREE SETUP - No API keys required!
# Copy this file to .env

# =============================================================================
# LLM Configuration - Using Ollama (FREE, LOCAL)
# =============================================================================

# Default: Ollama running locally on your machine
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2

# For M1 Mac, recommended models (in order of size/quality):
# - llama3.2 (2B params, fast, good for simple Q&A)
# - llama3.2:3b (3B params, better quality)
# - mistral (7B params, excellent quality)
# - llama3.1:8b (8B params, best quality, slower)

# Ollama URL (default for local development)
OLLAMA_BASE_URL=http://localhost:11434

# For Docker, use: OLLAMA_BASE_URL=http://host.docker.internal:11434

# =============================================================================
# Alternative: Cloud Providers (Optional - require API keys)
# =============================================================================

# Option 2: Groq (Free tier available)
# LLM_PROVIDER=groq
# LLM_MODEL=llama-3.3-70b-versatile
# GROQ_API_KEY=your_groq_api_key_here

# Option 3: OpenAI (Paid)
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# Speech-to-Text Configuration (FREE - Runs locally)
# =============================================================================

# Whisper model size: tiny, base, small, medium, large-v3
# For M1 Mac: 'base' or 'small' recommended
STT_MODEL=base

# Device: cpu (M1 uses CPU efficiently via Metal)
STT_DEVICE=cpu

# Quantization: int8 for speed, float32 for accuracy
STT_COMPUTE_TYPE=int8

# =============================================================================
# Text-to-Speech Configuration (FREE - Microsoft Edge TTS)
# =============================================================================

TTS_ENABLED=true
TTS_VOICE=en-US-AriaNeural
TTS_RATE=+0%

# =============================================================================
# RAG Configuration
# =============================================================================

# Chunk size in characters (smaller = more precise, larger = more context)
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Number of chunks to retrieve per query
RETRIEVAL_K=4

# Embedding model (runs locally, free)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# Session Configuration
# =============================================================================

SESSION_TIMEOUT_MINUTES=30
MAX_HISTORY_LENGTH=10

# =============================================================================
# Application Settings
# =============================================================================

DEBUG=false
CORS_ORIGINS=["*"]

# Knowledge base directory
KNOWLEDGE_DIRECTORY=./knowledge

# Vector store persistence
CHROMA_PERSIST_DIRECTORY=./data/chroma
